# -*- coding: utf-8 -*-
import gradio as gr
import torch
import os
from glob import glob
from pathlib import Path
from typing import Optional
from diffusers import StableVideoDiffusionPipeline
from diffusers.utils import load_image, export_to_video
from PIL import Image
import uuid
import random
from huggingface_hub import hf_hub_download
import time
from tenacity import retry, stop_after_attempt, wait_exponential
from tqdm.auto import tqdm

# ØªØ¹Ø±ÙŠÙ Ø£Ù‚ØµÙ‰ Ù‚ÙŠÙ…Ø© ØµØ­ÙŠØ­Ø© 64 Ø¨Øª
max_64_bit_int = 9223372036854775807  # 2**63 - 1

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± CUDA
device = "cuda" if torch.cuda.is_available() else "cpu"

def download_progress_callback(progress_bar):
    def callback(evolution: float):
        progress_bar.update(evolution - progress_bar.n)
    return callback

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def load_model():
    try:
        progress_bar = tqdm(total=100, desc="Downloading model")
        pipe = StableVideoDiffusionPipeline.from_pretrained(
            "stabilityai/stable-video-diffusion-img2vid-xt",
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            variant="fp16" if device == "cuda" else None,
            resume_download=True,  # Enable resuming interrupted downloads
            local_files_only=False,
            progress_callback=download_progress_callback(progress_bar)
        )
        progress_bar.close()
        return pipe
    except Exception as e:
        print(f"Error loading model: {str(e)}")
        raise

# ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¹ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©
try:
    pipe = load_model()
    pipe.to(device)
except Exception as e:
    print(f"Failed to load model after retries: {str(e)}")
    raise

# ØªÙØ¹ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
if hasattr(pipe, 'enable_model_cpu_offload'):
    pipe.enable_model_cpu_offload()
if hasattr(pipe, 'enable_vae_slicing'):
    pipe.enable_vae_slicing()
if hasattr(pipe, 'enable_vae_tiling'):
    pipe.enable_vae_tiling()

# ØªÙØ¹ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª CUDA Ø§Ù„Ù…Ø­Ø³Ù†Ø©
if device == "cuda":
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

# ØªØ¹Ø¯ÙŠÙ„ Ø¯Ø§Ù„Ø© build_scene_prompt Ù„ØªØ³ØªÙ‚Ø¨Ù„ ÙˆØµÙØ§Ù‹ ÙˆØ§Ø­Ø¯Ø§Ù‹
def build_scene_prompt(scene_description: str) -> str:
    """
    ØªØ­ÙˆÙŠÙ„ ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ù…Ù†Ø§Ø³Ø¨
    """
    return scene_description.strip() if scene_description else "Ù…Ø´Ù‡Ø¯ Ø¹Ø§Ù…"

# ØªØ¹Ø¯ÙŠÙ„ Ø¯Ø§Ù„Ø© sample Ù„ØªØ³ØªÙ‚Ø¨Ù„ ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ ÙƒÙ…Ø¹Ø§Ù…Ù„ ÙˆØ§Ø­Ø¯
def sample(
    image: Image,
    seed: Optional[int] = 42,
    randomize_seed: bool = True,
    motion_bucket_id: int = 180,
    num_frames: int = 45,
    fps_id: int = 8,
    scene_description: str = "",  # ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠ
    version: str = "svd_xt",
    cond_aug: float = 0.03,
    decoding_t: int = 3,
    device: str = device,
    output_folder: str = "outputs",
    progress=gr.Progress(track_tqdm=True)
):
    if image.mode == "RGBA":
        image = image.convert("RGB")
        
    if randomize_seed:
        seed = random.randint(0, max_64_bit_int)
    generator = torch.manual_seed(seed)
    
    os.makedirs(output_folder, exist_ok=True)
    base_count = len(glob(os.path.join(output_folder, "*.mp4")))
    video_path = os.path.join(output_folder, f"{base_count:06d}.mp4")

    # ØªØ­Ø¯ÙŠØ« ÙƒÙŠÙÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯
    scene_prompt = build_scene_prompt(scene_description)
    print(f"ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯: {scene_prompt}")  # Ù„Ù„ØªØµØ­ÙŠØ­

    # Fix the deprecation warning for torch.cuda.amp.autocast
    with torch.amp.autocast('cuda' if device == "cuda" else 'cpu'):
        frames = pipe(
            image,
            decode_chunk_size=decoding_t,
            generator=generator,
            motion_bucket_id=motion_bucket_id,
            noise_aug_strength=cond_aug,
            num_frames=num_frames,
            num_inference_steps=25
        ).frames[0]
    
    export_to_video(frames, video_path, fps=fps_id)
    return video_path, seed

def resize_image(image, output_size=(1024, 1024)):
    if image is None:
        return None
        
    target_aspect = output_size[0] / output_size[1]
    image_aspect = image.width / image.height

    if image_aspect > target_aspect:
        new_height = output_size[1]
        new_width = int(new_height * image_aspect)
        resized_image = image.resize((new_width, new_height), Image.LANCZOS)
        left = (new_width - output_size[0]) / 2
        top = 0
        right = (new_width + output_size[0]) / 2
        bottom = output_size[1]
    else:
        new_width = output_size[0]
        new_height = int(new_width / image_aspect)
        resized_image = image.resize((new_width, new_height), Image.LANCZOS)
        left = 0
        top = (new_height - output_size[1]) / 2
        right = output_size[0]
        bottom = (new_height + output_size[1]) / 2

    cropped_image = resized_image.crop((left, top, right, bottom))
    return cropped_image

custom_css = """
.gradio-logo, .footer, .gr-prose {
    display: none !important;
}
footer {
    display: none !important;
}
"""

with gr.Blocks(css=custom_css) as demo:
    gr.Markdown('''# Ù…Ø­ÙˆÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ - v6
    ØªØ·ÙˆÙŠØ±: Ø­Ø³Ø§Ù… ÙØ¶Ù„ Ù‚Ø¯ÙˆØ±
    
    Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:
    1. Ù‚Ù… Ø¨Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø©,ÙˆØµÙ ØªÙØµÙŠÙ„ÙŠ Ù„Ù„Ù…Ø´Ù‡Ø¯ (Ø¥Ø¬Ø¨Ø§Ø±ÙŠ)
    2. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
    3. Ø§Ø¶Ø¨Ø· Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©
    4. ,Ù…Ù„Ø§Ø­Ø¸Ø© Ù…Ù‡Ù…Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ø®Ø±ÙˆØ¬ ØªØ­ÙØ¸ ÙÙŠ Ù…Ø¬Ù„Ø¯ Ù…Ø¨Ø§Ø´Ø± Ø§ØªÙˆÙ…Ø§ØªÙŠÙƒÙŠ outputs Ø§Ù†Ù‚Ø± Ø¹Ù„Ù‰ Ø²Ø± ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ âœ¨
    ''')
    
    with gr.Row():
        with gr.Column():
            image = gr.Image(label="Ù‚Ù… Ø¨Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø©", type="pil")
            generate_btn = gr.Button("âœ¨ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ", variant="primary")
            clear_button = gr.Button("ğŸ—‘ï¸ Ù…Ø³Ø­", variant="secondary")
        video = gr.Video()
        
    with gr.Accordion("ğŸ› ï¸ Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©", open=False):
        seed = gr.Slider(
            label="Ø§Ù„Ø¨Ø°Ø±Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©",
            value=42,
            randomize=True,
            minimum=0,
            maximum=max_64_bit_int,
            step=1
        )
        randomize_seed = gr.Checkbox(label="Ø¨Ø°Ø±Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©", value=True)
        motion_bucket_id = gr.Slider(
            label="Ø´Ø¯Ø© Ø§Ù„Ø­Ø±ÙƒØ©",
            info="Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ù…Ù‚Ø¯Ø§Ø± Ø§Ù„Ø­Ø±ÙƒØ© Ø§Ù„Ù…Ø¶Ø§ÙØ©/Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©",
            value=250,
            minimum=1,
            maximum=500
        )
        num_frames = gr.Slider(
            label="Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª",
            info="Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø±Ø§Ø¯ ØªÙˆÙ„ÙŠØ¯Ù‡Ø§",
            value=20,
            minimum=10,
            maximum=60
        )
        fps_id = gr.Slider(
            label="Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠØ©",
            info="Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø³Ø±Ø¹Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (Ù…Ø¹Ø¯Ù„ Ø¥Ø·Ø§Ø±Ø§Øª Ø£Ù‚Ù„ = ÙÙŠØ¯ÙŠÙˆ Ø£Ø·ÙˆÙ„)",
            value=4,
            minimum=1,
            maximum=30
        )
      
    with gr.Accordion("ğŸ¬ ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯", open=True):
        scene_description = gr.Textbox(
            label="ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯",
            placeholder="Ø§ÙƒØªØ¨ ÙˆØµÙØ§Ù‹ ØªÙØµÙŠÙ„ÙŠØ§Ù‹ Ù„Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨. Ù…Ø«Ø§Ù„: Ø±Ø¬Ù„ ÙŠØ±ØªØ¯ÙŠ Ø¨Ø¯Ù„Ø© Ø±Ø³Ù…ÙŠØ© ÙŠÙ…Ø´ÙŠ ÙÙŠ Ø´Ø§Ø±Ø¹ Ù…Ø²Ø¯Ø­Ù… ØªØ­Øª Ø¥Ø¶Ø§Ø¡Ø© Ø§Ù„Ù…Ø³Ø§Ø¡ Ù…Ø¹ Ø­Ø±ÙƒØ© ÙƒØ§Ù…ÙŠØ±Ø§ Ø¨Ø·ÙŠØ¦Ø© Ù…Ù† Ø§Ù„ÙŠØ³Ø§Ø± Ù„Ù„ÙŠÙ…ÙŠÙ†",
            info="ÙˆØµÙ ØªÙØµÙŠÙ„ÙŠ Ù„Ù„Ù…Ø´Ù‡Ø¯ (Ø¥Ø¬Ø¨Ø§Ø±ÙŠ)",
            lines=3,
            value=""
        )

    image.upload(
        fn=resize_image,
        inputs=image,
        outputs=image,
        queue=False
    )
    
    def validate_inputs(image, description):
        if image is None:
            raise gr.Error("ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ ØµÙˆØ±Ø© Ø£ÙˆÙ„Ø§Ù‹")
        if not description.strip():
            raise gr.Error("ÙŠØ±Ø¬Ù‰ ÙƒØªØ§Ø¨Ø© ÙˆØµÙ Ù„Ù„Ù…Ø´Ù‡Ø¯")
        return True

    generate_btn.click(
        fn=validate_inputs,
        inputs=[image, scene_description],
        outputs=None,
        api_name=None,
    ).success(
        fn=sample,
        inputs=[
            image, seed, randomize_seed, motion_bucket_id,
            num_frames, fps_id, scene_description
        ],
        outputs=[video, seed],
        api_name="video"
    )

    # Ø¥Ø¶Ø§ÙØ© Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ù„ÙƒÙŠØ©
    gr.HTML(
        '<div class="custom-footer">Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ù…Ø­ÙÙˆØ¸Ø© Â© 2024 ØªØ·ÙˆÙŠØ±: Ø­Ø³Ø§Ù… ÙØ¶Ù„ Ù‚Ø¯ÙˆØ±</div>'
    )

    # Add this after creating the clear_button
    def clear_outputs():
        return None, None, ""  # Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø­ ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯

    clear_button.click(
        fn=clear_outputs,
        inputs=[],
        outputs=[image, video, scene_description],
        queue=False
    )

if __name__ == "__main__":
    demo.launch(
        share=False,
        server_port=7861,
        show_api=False
    )